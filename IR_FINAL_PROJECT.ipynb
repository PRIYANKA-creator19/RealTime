{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "QsiCXsqOBT_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = '/content/twitter_traffic_data_static.csv'\n",
        "pf_df = pd.read_csv(csv_file)"
      ],
      "metadata": {
        "id": "5pCXHKyQf86H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pf_df.head()"
      ],
      "metadata": {
        "id": "NV09VjIJgOU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pf_df['target'] = np.where(pf_df['text']==4, 0, 1)\n",
        "pf_df = pf_df.drop(columns=['text', 'class'])"
      ],
      "metadata": {
        "id": "hwlRwU1hgRyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(pf_df, test_size=0.3, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n"
      ],
      "metadata": {
        "id": "gH8hx-fMglQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tweepy\n",
        "!pip install elasticsearch\n",
        "from elasticsearch import Elasticsearch\n",
        "import requests\n",
        "import re"
      ],
      "metadata": {
        "id": "j7pZnRdghgNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ACCESS_TOKEN = '982440453936242688-cxHysxbD2RAxuamp7GLv2O1uaW75hlb'\n",
        "ACCESS_SECRET = '11wheFPpJSsHbAuh0veSBKI7Bz6t06mZvewaEjgUii3Tn'\n",
        "CONSUMER_KEY = '6sRDHYQ5gPPmSspaqhnPgzErS'\n",
        "CONSUMER_SECRET = '1kLAwSt9YCUlovHgEOgo0EOvkIflxM3E3tMmnaS5KCLhSahLyr'\n",
        "\n",
        "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "headers = {'content-type': 'application/json'}\n",
        "elasticsearch_index_uri = 'http://elastic:KWKWmZTobKtc3WsjVwWB@localhost:9200/twitter_data_mining/tweet'\n",
        "mapping = {\n",
        "    \"mappings\": {\n",
        "        \"tweet\": {\n",
        "            \"properties\": {\n",
        "                \"text\": {\n",
        "                    \"type\": \"keyword\"\n",
        "                },\n",
        "                \"timestamp\": {\n",
        "                    \"type\": \"date\",\n",
        "                    \"format\": \"yyyy-MM-dd HH:mm:ss\"\n",
        "                },\n",
        "            }\n",
        "        }\n",
        "     }\n",
        "}"
      ],
      "metadata": {
        "id": "sVghrcSnhmxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# es=Elasticsearch(http_auth=('elastic', 'KWKWmZTobKtc3WsjVwWB'))\n",
        "es=Elasticsearch(['http://localhost:9200/'], http_auth=('elastic', 'KWKWmZTobKtc3WsjVwWB'))\n",
        "# es.indices.delete(index='twitter_data_mining')\n",
        "# es.indices.create(index='twitter_data_mining', body=mapping, ignore=400)"
      ],
      "metadata": {
        "id": "3EIexsYajW_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kafka-python"
      ],
      "metadata": {
        "id": "c-Gh-olbA0sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install confluent-kafka"
      ],
      "metadata": {
        "id": "8sLf8C3YwsNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# access_token ='YOUR ACCESS TOKEN'\n",
        "# access_token_secret = 'YOUR SECRET ACCESS TOKEN'\n",
        "# consumer_key =  'YOUR CONSUMER KEY'\n",
        "# consumer_secret =  'YOUR SECRET CONSUMER KEY'\n",
        "\n",
        "# hashtags=[\"traffic nyc\",\"#traffic #nyc\", \"#traffic #ny\", \"#traffic #newyorkcity\", \"#traffic #newyork\", \"#accident #newyorkcity\",\n",
        "#           \"#roadblock #newyorkcity\", \"#accident #newyork\",\"#roadblock #newyork\", \"#accident #nyc\",\n",
        "#           \"#roadblock #nyc\", \"#accident #ny\",\"#roadblock #ny\"]\n",
        "# #hashtags=[\"#traffic #cleaveland\",\"#traffic #cleaveland\", \"#traffic #cleaveland\", \"#traffic #accident #cleaveland\",\"#traffic #roadblock #cleaveland\"]\n",
        "# #hashtags=[\"#traffic\", \"#traffic #accident\",\"#traffic #roadblock\"]"
      ],
      "metadata": {
        "id": "TTSt63drvzKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "# Download Java Virtual Machine (JVM)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Download Spark\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# Unzip the file\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.2.1-bin-hadoop3.2'\n",
        "!ls\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Check Spark Session Information\n",
        "spark\n",
        "from pyspark import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "\n"
      ],
      "metadata": {
        "id": "KMeQfHIjntiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install tensorflow-io\n",
        "!pip install elasticsearch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "\n",
        "\n",
        "csv_file = '/content/twitter_traffic_data_static.csv'\n",
        "pf_df = pd.read_csv(csv_file)\n",
        "pf_df.head()\n",
        "pf_df['target'] = np.where(pf_df['text']==4, 0, 1)\n",
        "pf_df = pf_df.drop(columns=['text', 'class'])\n",
        "train_df, test_df = train_test_split(pf_df, test_size=0.3, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))"
      ],
      "metadata": {
        "id": "GvJxuYaZmh6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
        "\n",
        "# Load the CSV file into a Spark DataFrame\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/content/twitter_traffic_data_static.csv\")\n",
        "\n",
        "# Show the first few rows of the DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "BbstcnJsm5Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing"
      ],
      "metadata": {
        "id": "i8gWPxmioWwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, lower, trim\n",
        "\n",
        "# Replace any non-alphanumeric characters in the text column with a space\n",
        "df = df.withColumn(\"text\", regexp_replace(\"text\", \"[^a-zA-Z0-9\\\\s]\", \" \"))\n",
        "\n",
        "# Convert the text column to lowercase\n",
        "df = df.withColumn(\"text\", lower(\"text\"))\n",
        "\n",
        "# Remove leading and trailing whitespace from the text column\n",
        "df = df.withColumn(\"text\", trim(\"text\"))\n",
        "\n",
        "# Show the updated DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "PNib14WfnuvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Indexing"
      ],
      "metadata": {
        "id": "XmNeOgAyonqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Create a StringIndexer to encode the class column as numeric values\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
        "\n",
        "# Fit the StringIndexer to the DataFrame\n",
        "indexer_model = indexer.fit(df)\n",
        "\n",
        "# Transform the DataFrame to add the encoded label column\n",
        "indexed_df = indexer_model.transform(df)\n",
        "\n",
        "# Show the updated DataFrame\n",
        "indexed_df.show()\n"
      ],
      "metadata": {
        "id": "EpD1Yx1LoaLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Group the DataFrame by the class column and count the number of occurrences of each class\n",
        "class_counts = df.groupBy(\"class\").agg(count(\"*\").alias(\"count\"))\n",
        "\n",
        "# Convert the class_counts DataFrame to a Pandas DataFrame for plotting\n",
        "class_counts_pd = class_counts.toPandas()\n",
        "\n",
        "# Create a bar chart of the class distribution\n",
        "fig, ax = plt.subplots()\n",
        "class_counts_pd.plot(kind=\"bar\", x=\"class\", y=\"count\", ax=ax, legend=False)\n",
        "ax.set_xlabel(\"Class\")\n",
        "ax.set_ylabel(\"Count\")\n",
        "ax.set_title(\"Class Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l0nvPRxBorHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Building"
      ],
      "metadata": {
        "id": "KMeTF6bqpuS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "\n",
        "# split the data into training and testing sets\n",
        "train, test = indexed_df.randomSplit([0.8, 0.2], seed=12345)\n",
        "# tokenize the text column\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
        "train = tokenizer.transform(train)\n",
        "test = tokenizer.transform(test)\n",
        "\n",
        "# create a feature vector using the HashingTF transformer\n",
        "hashingTF = HashingTF(inputCol='words', outputCol='features')\n",
        "train = hashingTF.transform(train)\n",
        "test = hashingTF.transform(test)\n",
        "# train a Naive Bayes model\n",
        "nb = NaiveBayes(smoothing=1.0, modelType='multinomial', labelCol='label', featuresCol='features')\n",
        "model = nb.fit(train)\n",
        "\n",
        "# make predictions on the testing set\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# evaluate the model's performance\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Accuracy: {}'.format(accuracy))\n"
      ],
      "metadata": {
        "id": "geRuThC3o5IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "\n",
        "train, test = indexed_df.randomSplit([0.8, 0.2], seed=42)\n"
      ],
      "metadata": {
        "id": "8Dxie1wmr83A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# define the pipeline stages\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
        "hashing_tf = HashingTF(inputCol='words', outputCol='raw_features')\n",
        "idf = IDF(inputCol='raw_features', outputCol='features')\n",
        "dt = DecisionTreeClassifier(labelCol='label', featuresCol='features')\n",
        "pipeline = Pipeline(stages=[tokenizer, hashing_tf, idf, dt])\n",
        "\n",
        "# fit the pipeline to the training data\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# make predictions on the test data\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# evaluate the model's performance\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Accuracy: {}'.format(accuracy))\n"
      ],
      "metadata": {
        "id": "9q7iKazHFNW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_names = ['Decision Tree', 'Naive Bayes']\n",
        "accuracies = [0.952991, 0.9116]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(model_names, accuracies)\n",
        "\n",
        "for i, v in enumerate(accuracies):\n",
        "    ax.text(i, v+0.01, str(v), color='black', fontweight='bold', ha='center')\n",
        "\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Comparison of Models')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "13r17CL2FuNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load data from CSV file\n",
        "label_df = indexed_df.toPandas()\n",
        "print(label_df.head(5))\n",
        "data = label_df\n",
        "# Preprocess tweets\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    tweet = re.sub(r'http\\S+', '', tweet) # Remove URLs\n",
        "    tweet = re.sub(r'@\\S+', '', tweet) # Remove mentions\n",
        "    tweet = re.sub(r'#\\S+', '', tweet) # Remove hashtags\n",
        "    tweet = re.sub(r'\\d+', '', tweet) # Remove numbers\n",
        "    tweet = re.sub(r'[^\\w\\s]', '', tweet) # Remove punctuation\n",
        "    tweet = tweet.lower() # Convert to lowercase\n",
        "    words = tweet.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    tweet = ' '.join(words)\n",
        "    return tweet\n",
        "\n",
        "data['text'].apply(preprocess_tweet_text)\n",
        "\n",
        "# Extract features using TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
        "X = vectorizer.fit_transform(data['text'])\n",
        "y = data['label']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression model\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = clf.predict(X_test)\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Precision:', precision_score(y_test, y_pred,average='macro'))\n",
        "print('Recall:', recall_score(y_test, y_pred,average='macro'))\n",
        "print('F1-score:', f1_score(y_test, y_pred,average='macro'))\n"
      ],
      "metadata": {
        "id": "D1TOAuQsymwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL TUNING OF NAIVEBAYES CLASSIFIER"
      ],
      "metadata": {
        "id": "hUpKjCuZ7sAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# split the data into training and testing sets\n",
        "train, test = indexed_df.randomSplit([0.8, 0.2], seed=12345)\n",
        "\n",
        "# tokenize the text column\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
        "train = tokenizer.transform(train)\n",
        "test = tokenizer.transform(test)\n",
        "\n",
        "# create a feature vector using the HashingTF transformer\n",
        "hashingTF = HashingTF(inputCol='words', outputCol='features')\n",
        "train = hashingTF.transform(train)\n",
        "test = hashingTF.transform(test)\n",
        "\n",
        "# define the Naive Bayes model\n",
        "nb = NaiveBayes(smoothing=1.0, modelType='multinomial', labelCol='label', featuresCol='features')\n",
        "\n",
        "# define the parameter grid to search over\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(nb.smoothing, [0.5, 1.0, 2.0]) \\\n",
        "    .build()\n",
        "\n",
        "# use cross-validation to find the best set of hyperparameters\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
        "cv = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, seed=12345)\n",
        "cvModel = cv.fit(train)\n",
        "\n",
        "# make predictions on the testing set using the best model\n",
        "predictions = cvModel.transform(test)\n",
        "\n",
        "# evaluate the model's performance\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print('Accuracy: {}'.format(accuracy))\n",
        "\n",
        "# save the trained model to disk\n",
        "cvModel.bestModel.save('naive_bayes_model')\n",
        "\n",
        "# load the saved model from disk\n",
        "from pyspark.ml.classification import NaiveBayesModel\n",
        "model = NaiveBayesModel.load('naive_bayes_model')\n",
        "\n",
        "# prepare new data for prediction\n",
        "from pyspark.sql.functions import col\n",
        "new_data = spark.createDataFrame([(1, 'this is some text')], ['id', 'text'])\n",
        "new_data = tokenizer.transform(new_data)\n",
        "new_data = hashingTF.transform(new_data)\n",
        "\n",
        "# make predictions using the loaded model\n",
        "predictions = model.transform(new_data)"
      ],
      "metadata": {
        "id": "ElrUo64iw-si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the CSV file containing Twitter tweets\n",
        "tweets_df = pd.read_csv('/content/twitter_traffic_data_static.csv')\n",
        "# Preprocess the text data by removing special characters, converting to lowercase, etc.\n",
        "tweets_df['text'] = tweets_df['text'].apply(lambda x: ' '.join(word.lower() for word in x.split() if word.isalpha()))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets_df['text'], tweets_df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer object to convert text into numerical features\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Fit the TfidfVectorizer to the training data and transform both the training and testing data\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "# Create an SVM model and fit it to the training data\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Use the trained model to make predictions on the testing data\n",
        "y_pred = svm.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the performance of the model using accuracy score and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "WISk2OowSUDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEEP LEARNING MODEL WITH NEW DATA CLASSIFICATION"
      ],
      "metadata": {
        "id": "Mh3cvC3H6ekl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load the labeled data\n",
        "df = pd.read_csv('/content/twitter_traffic_data_static.csv')\n",
        "\n",
        "# Convert the class labels to numeric format\n",
        "le = LabelEncoder()\n",
        "df['class'] = le.fit_transform(df['class'])\n",
        "\n",
        "# Balance the classes using oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(df['text'].values.reshape(-1, 1), df['class'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(len(X_resampled) * 0.8)\n",
        "train_text = X_resampled[:train_size].flatten()\n",
        "train_labels = y_resampled[:train_size]\n",
        "val_text = X_resampled[train_size:].flatten()\n",
        "val_labels = y_resampled[train_size:]\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "# Convert the text data into sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_text)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "max_length = 50\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Define the deep learning model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=32, input_length=max_length),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "    tf.keras.layers.LSTM(128),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=1e-4), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_padded, train_labels, validation_data=(val_padded, val_labels), epochs=20, batch_size=32)\n",
        "\n",
        "# Evaluate the model on new data\n",
        "new_text = ['rushhour','walking on the sidewalk']\n",
        "new_sequences = tokenizer.texts_to_sequences(new_text)\n",
        "new_padded = pad_sequences(new_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "predictions = model.predict(new_padded)\n",
        "\n",
        "# Convert the predictions back to class labels\n",
        "class_labels = le.inverse_transform(np.round(predictions).astype(int))\n",
        "\n",
        "# Print the predictions\n",
        "for i in range(len(new_text)):\n",
        "    print(new_text[i] + ' is ' + class_labels[i])"
      ],
      "metadata": {
        "id": "7Dgz3aijsLNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set\n",
        "accuracy = history.history['accuracy'][-1]\n",
        "\n",
        "print('Validation accuracy:', accuracy)\n",
        "print(\"ML Model Performance\")\n",
        "print(\"Accuracy:\",accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "-DR8W5ymO-Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = history.history['accuracy'][-1]\n",
        "accuracy"
      ],
      "metadata": {
        "id": "Z8NEyW_RQOyX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}